



[TOC]

## 3.5 

### Linear Independence

#### Definition: 

- The columns of A are ***linearly indenpendent*** when the only solution to $Ax = 0$ is $x = 0$. ***No other combination*** $Ax$ of the columns gives the zero vector.


- ​

$$
x_1v_1 + x_2v_2 + ... + x_nv_n = 0
$$

only happens when all $x^`s$ are zero.

#### 推论：

1. The columns of $A$ are independent exactly when the rank is $r=n$. There aer n pivots and no free variables. Only $x=0$ is in the nullspace.
2. Any set of $n$ vectors in $R^m$ must be linearly dependent if $n>m$.

### Span a subspace

#### Definition:

- A set of vectors ***spans*** a space of their linear combinations fill the space.
- The ***row space*** of a matirx is the subspace of $R^n$ spanned by rows.

### Basis for a vector space

#### Definition:

- A ***basis*** for a vector space its asequence of vectors that has two properties at once:
    1. The vecotes are ***linearly independent***.
    2. The vectors ***span the space***.


**There is one and only one way to write $v$ as a combination of the basis vectors**.

The columns of any *invertible* n by n matrix give a basis for $R^n$.

#### 推论:

1. The vectors $v_1, …, v_n$ are a basis for $R^n$ exactly when they are ***the columns of an n by n invertible matrix***. Thus $R^n$ has infinitely many different bases.
2. ***The pivot columns of A are a basis for its column space***. The pivot rows of $A$ are a basis for its row space. So are the pivot rows of its echelon form $R$.

### Dimension of a space

#### Definition:

- The ***dimension of a space*** is the number of vectors in every basis.

### Bases for matrix spaces and function spaces:

#### Matrix spaces:

**Example:** The vector space $M$ contains all 2 by 2 matrices. Its dimension is 4.

One basis is:

$$
{A_1},{A_2},{A_3},{A_4} = \left[ {\matrix{
   1 & 0  \cr 
   0 & 0  \cr 

 } } \right],\left[ {\matrix{
   0 & 1  \cr 
   0 & 0  \cr 

 } } \right],\left[ {\matrix{
   0 & 0  \cr 
   1 & 0  \cr 

 } } \right],\left[ {\matrix{
   0 & 0  \cr 
   0 & 1  \cr 

 } } \right]
$$
they span the space:
$$
c_1A_1 + c_2A_2 + c_3A_3 + c_4A_4 = 
\left[ {\matrix{
   c_1 & c_2  \cr 
   c_3 & c_4  \cr 

 } } \right]
$$
This is zero only if the $c^`$s are all zero — which proves independence.

> The dimension of the whole $n$ by $n$ matrix space is $n^2$.
>
> The dimension of the subspace of upper triangular matrices is ${1 \over 2}{n^2} + {1 \over 2}n$.
>
> The dimension of the subspace of diagonal matrices is n.

#### Function space:

**Example:**

> $y'' = 0$ is solved by any linear function $y = cx + d$
>
> $y'' = -y$ is solved by any combination $y = c sinx + d cosx$
>
> $y'' = y$ is solved by any combination $y=c e^x + de^{-x}$

The first space has $x$ and 1, it is the 'nullspace' of the second derivative. The second solution space has two basis functions: $sinx$ and $cosx$. The third solution space has basis functions $e^x$ and $e^{-x}$.

## 3.6

> Four fundamental subspaces:
>
> > 1. The ***row space*** is $C(A^T)$, a subspace of $R^n$.
> > 2. The ***column space*** is $C(A)$, a subspace of $R^m$.
> > 3. The ***nullspace*** is $N(A)$, a subspace of $R^n$.
> > 4. The ***left nullspace*** is $N(A^T)$, a subspace of $R^m$. 

### The four subspaces for $R$:

1. In $R^n$ ***the row space*** and ***nullspace*** have dimensions $r$ and $n-r$.
2. In $R^m$ ***the column space*** and ***left nullspace*** have dimensions $r$ and $m-r$.

### Fundamental theorem of linear algebra, Part 1:

- The column space and row space both have dimension $r$.
- The nullspaces have dimensions $n-r$ and $m-r$.

### Matrices of rank one:

- Every rank one matrix has the special form $A = uv^T$ = column times row.

example:

$$
A = \left[ {\matrix{
   1  \cr 
   2  \cr 
   { - 3}  \cr 
   0  \cr 

 } \matrix{
   2  \cr 
   4  \cr 
   { - 6}  \cr 
   0  \cr 

 } \matrix{
   3  \cr 
   6  \cr 
   { - 9}  \cr 
   0  \cr 

 } } \right]
$$
equals:

$$
\left[ {\matrix{
   1  \cr 
   2  \cr 
   { - 3}  \cr 
   0  \cr 

 } } \right] \times \left[ {\matrix{
   1 & 2 & 3  \cr 

 } } \right]
$$

# 4 Orthogonality

## 4.1

***The row space is perpendicular to the nullspace***.

### Orthogonality of the four subspaces

#### Definition:

- Two subspaces $V$ and $W$ of a vector space are ==**orthogonal**== if every vector $v$ in $V$ is perpendicular to every vector $w$ in $W$.

#### 推论：

- Every vector $x$ in the nullspace of A is perpendicular to every row of A, because $Ax=0$. ==***The nullspace and row space are orthogonal subspaces***==.


- Every vector $y$ in the nullspace of $A^T$ is perpendicular to every column of $A$. ==***The left nullspace and the column space are orthogonal in $R^m$***==.


### Orthogonal Complements

#### Definition:

- The ***prthogonal complement*** of $V$ contains ==every== vector that is perpendicular to $V$. This orthogonal subspace is denoted by $V^\bot$.

### Fundamental theorem of Linear algebra, part 2:

- The nullspace is the orthogonal complenment of the row space (in $R^n$)
- The left nullspace is the orthogonal complement of the column space (in $R^m$)

#### 推论：

- Every vector in the column sapce comes from one and only one vector $x_r$ in the row space.

### Combining bases from subspaces:

#### 推论：

- Any $n$ linearly independent vectors in $R^n$ must span $R^n$. They are a basis. Any $n$ vectors that span $R^n$ must be independent. They are basis.
- A is an n by n square matrix. If the n columns of A are independent, they span $R^n$, So Ax=b is solvable. If the n columns span $R^n$, they are independent. So Ax=b has only one solution. Then A is invertible.
- Every x in $R^n$ has a nullspace component $x_n$ and a row component $x_r$.

## 4.2 Projections

### Projection onto a line

$a$ is a line through the origin.

$b$ is a line through the origin.

The line from $b$ to $p$ is perpendicular to the vector $a$.

- How to find the projection matrices:

  1. find $$\hat x$$
  2. find the vector $p$
  3. find the matrix $P$.


  ![Screenshot 2016-05-30_13-57-37](/Users/cyh/Pictures/Apowersoft Mac Screenshot/Screenshot 2016-05-30_13-57-37.png)

#### 推论：

- The projection of $b$ to the line through $a$ is the vector $$p = \hat xa = {{{a^T}b} \over {{a^T}a}}a$$. 
  1. Special case 1: If $b=a$ then $$\hat x = 1$$. The projection of $a$ onto $a$ is itself.
  2. Special case 2: If $b$ is perpendicular to $a$ then $a^Tb=0$. The projection is $p=0$.
- $$p = a\hat x =a {{{a^T}b} \over {{a^T}a}}=Pb \text{    when the matrix is } P={{a{a^T}} \over {{a^T}a}}$$.
  - $P$ is a column times a row! The column is $a$, the row is $a^T$
  - $P$ is m by m, but its rank is 1.


- Projecting a second time doesn't change anything, $P^2 = P$.
- When matrix $P$ projects onto one subspace, matrix $I-P$ projects onto the perpendicular subspace.

### Projection onto a subspace

#### 推论：

- The combination $$\hat{x}_1a_1+…+\hat{x}_na_n=A\hat{x}$$ that closest to $b$ comes from $$A^T(b-A\hat{x})=0 \text{  or  } A^TA\hat{x}=A^Tb$$.
- The solution is $$\hat x = (A^TA)^{-1}A^Tb$$.
- The projection of b onto the subspace is the vector $$p = A\hat x = A(A^TA)^{-1}A^Tb$$. 

![Screenshot 2016-05-30_14-46-08](/Users/cyh/Pictures/Apowersoft Mac Screenshot/Screenshot 2016-05-30_14-46-08.png)

- $A^TA$ is invertible if and only if $A$ has linearly independent columns(all A's columns are independent).
  - When $A$ has independent columns, $A^TA$ is square, symmetric, and invertible.

## 4.3 Least squares approximations

### Minimizing the error

- by geometry
- by algebra
- by calculus

### The big picture

 ![Screenshot 2016-05-30_16-21-04](/Users/cyh/Pictures/Apowersoft Mac Screenshot/Screenshot 2016-05-30_16-21-04.png)

  ![QQ20160530-0](/Users/cyh/Pictures/qq/QQ20160530-0.png)

- $N(A)$ just one point, with independent columns, $A^TA$ is invertible.

### Fitting a straight line

- The closest line $C+Dt$ has heights $p_1,…,p_m$ with errors $e_1,…,e_m$.$$A^TA\hat x = A^T b$$ will gibe $$\hat x = (C, D)$$. The erroes are $$e_i = b_i -C -Dt_i$$.

 ![QQ20160530-1](/Users/cyh/Pictures/qq/QQ20160530-1.png)

 ![QQ20160530-3](/Users/cyh/Pictures/qq/QQ20160530-3.png) ![QQ20160530-2](/Users/cyh/Pictures/qq/QQ20160530-2.png)

#### 结论：

- The line $C+Dt$ which minimizes $e^2_1+…+e_m^2$ is determined by $A^TA\hat x=A^Tb$: 
$$
\left[ {\matrix{
   m & {\sum {{t_i}} }  \cr 
   {\sum {{t_i}} } & {\sum {t{{_{}^2}_i}} }  \cr 

 } } \right]\left[ {\matrix{
   C  \cr 
   D  \cr 

 } } \right] = \left[ {\matrix{
   {\sum {{b_i}} }  \cr 
   {\sum {{t_i}{b_i}} }  \cr 

 } } \right]
$$

### Fitting by a parabola

 ![QQ20160530-4](/Users/cyh/Pictures/qq/QQ20160530-4.png)

## 4.4 Orthogonal bases and Gram-Schmidt

#### Definition:

- The vectors $q_1,…,q_n$ are ***orthonormal*** if:

$$
q_i^T{q_j} = \left\{ {\matrix{
   0  \text{	when $i \ne j$  (orthogond vectors)}\cr 
   1  \text{	when $i=j$  (unit vectors:$\parallel q_i \parallel = 1$ )}\cr 

 } } \right.
$$

- A matrix with orthonormal columns is assigned the special letter $Q$. $Q$ is no needs to be square.

#### 结论：

- A matrix $Q$ with orthonormal columns satisfies $Q^TQ=I$. 
- If $Q$ is square, we call it ***orthogonal matrix***. The inverse is the transpose.
- If $Q$ has orthonormal columns ($Q^TQ=I$), it leaves lengths unchanged:

$$
\parallel Qx \parallel = \parallel x \parallel \text{ for every vector $x$}
$$

$$
(Qx)^T(Qy) = x^T Q^TQy=x^Ty
$$

### Projections using orthogonal bases: $Q$ replaces $A$

- The least squares solutions of $Qx=b$ is $\hat x = Q^Tb$. The projection matrix is $P = QQ^T$.
  ![QQ20160531-0](/Users/cyh/Pictures/qq/QQ20160531-0.png)

 ![QQ20160531-1](/Users/cyh/Pictures/qq/QQ20160531-1.png)

### The Gram-Schmidt process

start with three independent vectors $a, b, c$.

1. choosing $A=a$

2. start with $b$ and choose its projection along $A$, the leaving perpendicular part will be choosen as $B$.

    ![QQ20160531-3](/Users/cyh/Pictures/qq/QQ20160531-3.png)

3. start with $c$ and choose its projection along $AB$ plane, the leaving perpendicular part will be choose as $C$.

    ![QQ20160531-4](/Users/cyh/Pictures/qq/QQ20160531-4.png)

4. divide $A, B, C$ by their lengths.

  ![QQ20160531-2](/Users/cyh/Pictures/qq/QQ20160531-2.png)

- Key idea: ***subtract from every new vector ite projections in the directions already set***.

### The fractorization $A=QR$

 ![QQ20160531-5](/Users/cyh/Pictures/qq/QQ20160531-5.png)

#### 结论：

- From independent vectors $a_1, …, a_n$, Gram-Schmidt constructs orthonomail vectors $q_1, …, q_n$. The matrices with thest columns satisfy $A=QR$. Then $R=Q^TA$ is triangular because later $q's$ are orthogonal to eralier $a's$.







